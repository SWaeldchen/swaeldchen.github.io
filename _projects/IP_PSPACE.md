---
layout: page
title: Certifiable Game Plans
description: Can we certify the plan of a powerful but possibly adversarial AI?
img:
importance: 1
category: Alignment
---



This is my idea for a new AI alignment project. Basically, I want to answer the followign question:

<blockquote style="text-align:center; font-style:italic; color:#333;">
  "Can a more powerful reasoner convince a human auditor of the validity of a plan, even in a complex environment?"
</blockquote>

This point was recently raised by Max Tegmark in his Lex Fridman [interview](https://www.youtube.com/watch?v=VcVfceTsD0A?t=1h46m00s).
