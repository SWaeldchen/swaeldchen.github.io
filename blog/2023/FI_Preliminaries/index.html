<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Interactive Classification 1: Preliminaries | Stephan T. Wäldchen</title> <meta name="author" content="Stephan T. Wäldchen"> <meta name="description" content="Stephan Wäldchen - Researcher in Interpretable AI - This is my personal blog about new research, announcements and thoughts. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/styles.css"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://swaeldchen.github.io/blog/2023/FI_Preliminaries/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Stephan </span>T. Wäldchen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Science</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">Talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/art/">Art</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Interactive Classification 1: Preliminaries</h1> <p class="post-meta">January 2, 2023• Stephan Wäldchen</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/interpretability"> <i class="fas fa-hashtag fa-sm"></i> interpretability</a>   <a href="/blog/tag/feature-importance-attribution"> <i class="fas fa-hashtag fa-sm"></i> feature importance attribution</a>   <a href="/blog/tag/fia"> <i class="fas fa-hashtag fa-sm"></i> fia</a>   <a href="/blog/tag/modelling-problem"> <i class="fas fa-hashtag fa-sm"></i> modelling problem</a>     ·   <a href="/blog/category/research"> <i class="fas fa-tag fa-sm"></i> research</a>   </p> </header> <article class="post-content"> <p>This post is part 1 of my series on <a href="/blog/2023/FI_start/">Interactive Classification</a>.</p> <p><strong>TL;DR:</strong> We present an overview of the current approaches and hurdles for formal interpretability:</p> <ol> <li>Feature Importance Attribution</li> <li>Shapley Values, Prime Implicants and Maximum Mutual Information</li> <li>The Modelling Problem and Manipulating Explanations </li> </ol> <style>.figcap{font-size:.9em}</style> <h3 id="interpretable-ai">Interpretable AI</h3> <p>An interpretable AI systems allows the human user to understand its reasoning process. Examples are decision trees, sparse lnear models and \(k\)-nearest neighbors.</p> <p>The standard bearer of modern machine learning, the <strong>Neural Networks</strong>, while achieving unprecedented accuracy, is nevertheless considered a <strong>black box</strong>, which means its reasoning is not made explicit. While we mathetically understand exactly what is happening in a single neuron, the interplay of thousands of these neurons results in behaviour that cannot be predicted straigtforward way. Compare this with how we exactly understand how an AND-gate and a NOT-gate work and how each program of finite length can be expressed as a series of these gates, yet we cannot understand a program just from reading the cuircuit plan.</p> <p>Interpretability research aims to remedy this fact by accompanying a decision, e.g. such as a classification, with addititional information that describes the reasoning process.</p> <p>One of the most prominent approaches is feature importance maps, which, for a given input, rate the input features for their imporatance to the model output.</p> <div style="display: flex; justify-content: center;"> <img src="https://swaeldchen.github.io/assets/img/merlin_arthur/decision_list.png" alt="img1" style="float:center; margin-right: 5%; width:95%"> <p style="clear: both;"></p> </div> <p class="figcap"><strong>Figure 1.</strong> An example of a decision list taken from <a class="citation" href="#rudin2019stop">(Rudin, 2019)</a> used to predict whether a delinquent will be arrested again. The reasoning of the decision list is directly readable.)</p> <h3 id="feature-importance-attribution">Feature Importance Attribution</h3> <p>For a given classifier and an input, <strong>feature importance attribution</strong> (FIA) or feature importance map aims to highlight what part of the input is relevant for the classifier decision on this input. The idea is that generally only a small part of the input is actually important. If for example a neural network decides whether an image contains a cat or a dog, only the part of the image displaying the respectiv animal should be considered important. This consideration omits in which way the important features were considered. It can thus be seen as the <strong>lowest level</strong> of the reasoning process.</p> <p>There are quite a lot of practical approaches that derive feature importance values for neural networks, see <a class="citation" href="#mohseni2021multidisciplinary">(Mohseni et al., 2021)</a>. However, these methods are defined purely <strong>heuristically</strong>. They come without any defined target properties for the produced attributions. Furthermore, it has been demonstrated that these methods can be <strong>manipulated</strong> by clever designs of the neural network.</p> <div style="display: flex; justify-content: center;"> <img src="https://swaeldchen.github.io/assets/img/merlin_arthur/lrp_example.png" alt="img1" style="float:center; margin-right: 5%; width:80%"> <p style="clear: both;"></p> </div> <p class="figcap"><strong>Figure 1.</strong> Feature importance map generated with LRP for a Fisher Vector Classifier (FV) and a Deep Neural Network (DNN). One can see that the FV decides the boat class based mostly on the water. Will this classifier generalise to boats without water? From <a class="citation" href="#lapuschkin2016analyzing">(Lapuschkin et al., 2016)</a>.</p> <h3 id="manipulation-of-heuristic-feature-importance">Manipulation of Heuristic Feature Importance</h3> <p>We are talking about manipulations in the follwing sense: Given that I have a neural network classifier \(\Phi\) that performs well for my purposes, I want another classifier \(\Phi^\prime\) that performs equally well but with a completely arbitrary feature importance.</p> <p>These heuristic FIAs all make implicit assumptions on the data distribution (some of them do that in a layer-wise fashion), see <a class="citation" href="#lundberg2017unified">(Lundberg &amp; Lee, 2017)</a>.</p> <p>All these heuristic explanation methods can be manipulated with the same trick: Basically keep the on-manifold behaviour constant, but change the off-manifold behaviour to influence the interpretations.</p> <p>Example from <a class="citation" href="#slack2020fooling">(Slack et al., 2020)</a>: \(\Phi\) is a discriminatory classifier, \(\Psi\) is a completely fair classifier and there is a helper function that decides if an input is on-manifold, belongs to a subspace of typical sample \(\mathcal{X}\). They define</p> \[\Phi^\prime(\mathbf{x}) = \begin{cases} \Phi(\mathbf{x}) &amp; \text{if}~ \mathbf{x}\in \mathcal{X}, \\ \Psi(\mathbf{x}) &amp; \text{otherwise.} \end{cases}\] <p>Now \(\Phi^\prime\) will almost always discriminate since for \(\mathbf{x}\) that lie on the manifold, whereas the explanations will be dominated by the fair classifier \(\Psi\), since most samples for the explanations are not on manifold. Thus the FIA highlights the innocuous features instead of the discriminatory ones.</p> <div style="display: flex; justify-content: center;"> <img src="https://swaeldchen.github.io/assets/img/merlin_arthur/off-manifold.png" alt="img1" style="width:50%"> </div> <p class="figcap"><strong>Figure 1.</strong> On-manifold data samples (blue) and off-manifold LIME-samples (red) for the COMPAS dataset; from <a class="citation" href="#dimanov2020you">(Dimanov et al., 2020)</a>.</p> <p>Formal approaches to interpretability thus need to make the underlying data distribution explicit.</p> <h3 id="formal-definition-of-feature-importance">Formal definition of feature importance</h3> <p>There are three main approaches to feature importance attribution:</p> <ol> <li>Shapley values</li> <li>Prime Implicants</li> <li>Maximal Mutual Information While Shapley values directly give a score to each feature, prime implicants and maximum mutual information select a subset of the features as <strong>the important features</strong>. However, since most search methods for these subsets work over convex relaxations of set membership, the unthresholded scores can serve as an importance value.</li> </ol> <p><strong>Shapley Values</strong> are a value attribution method from cooperative game theory. It is the unique are the unique method that satisfies the following desirable properties: linearity, symmetry, null player and efficiency <a class="citation" href="#shapley1997value">(Shapley, 1997)</a>. The idea is that a set of players achieve a common value. This value is to be fairly distributed to the players according to their importance. For this one considers every possible subset of players, called a coalition and the value this coalition would achieve. Thus, to define Shapley Values one needs a so called <em>characteristic function</em>, a value function that is defined on a set as well as all possible subsets. For \(d\) players, let \(\nu: 2^{[d]} \rightarrow \mathbb{R}\). Then \(\phi_i\), the Shapley value of the \(i\)-th player is defined as</p> \[\phi_{\nu,i} = \sum_{S \subseteq [d]\setminus\{i\}} \begin{pmatrix} d-1 \\ |S| \end{pmatrix}^{-1} ( \nu(S \cup \{i\}) - \nu(S) ).\] <p>Thus the Shapley values sum over all marginal contributions of the \(i\)-th player for ever possible coalition. In machine learning, the players correspond to features and the coalitions to subsets of the whole input. The explicite training of a characteristic function has been used in the context of simple two-player games to compare with heuristic attribution methods in <a class="citation" href="#waldchen2022training2">(Wäldchen et al., 2022)</a>. However, generally in machine learning, the model cannot evaluate subsets of inputs. For a given input \(\mathbf{x}\) and classification function \(f\), define \(\nu\) over expectation values:</p> \[\nu_{f,\mathbf{x}}(S) = \mathbb{E}_{\mathbf{y}\sim \mathcal{D}}[f(\mathbf{y})\,|\, \mathbf{y}_S = \mathbf{x}_S ] = \mathbb{E}_{\mathbf{y}\sim \mathcal{D}|_{\mathbf{x}_S}}[f(\mathbf{y})].\] <div style="display: flex; justify-content: center;"> <img src="https://swaeldchen.github.io/assets/img/merlin_arthur/Shapley.png" alt="img1" style="float:center; margin-right: 1%; width:100%"> <p style="clear: both;"></p> </div> <p class="figcap"><strong>Figure 1.</strong> Illustration of the idea of Shapley Values. For three players the pay-off for each possible coalition is shown. <a href="https://clearcode.cc/blog/game-theory-attribution/" rel="external nofollow noopener" target="_blank">Source</a></p> <p><strong>Prime Implicants</strong></p> <p>A series of appraoches considers hwo much a subset of the features of \(\mathbf{x}\) already determine the function output \(f(\mathbf{x})\). One of the most straight-forward approaches are the prime implicant explanations [D] for Boolean classifiers. An implicant is a part of the input that determines the output of the function completely, no matter which value the rest of the input takes. A prime implicant is an implicant that cannot be reduced further by omitting features.</p> <p>This concept is tricky to implement for very the highly non-linear neural networks, as small parts of an input can often be manipulated to give a completely different classification, see <a class="citation" href="#brown2017adversarial">(Brown et al., 2017)</a>. Thus, prime implicant explanations need to cover almost the whole input, and are thus not very informative.</p> <p>Probabilistic prime implicants have thus be introduced. As a relaxed notion, they only require the implicant to determine the function output with some high probability \(\delta\), see <a class="citation" href="#waldchen2021computational2">(Wäldchen et al., 2021)</a>, and in <a class="citation" href="#ribeiro2018anchors">(Ribeiro et al., 2018)</a> as <strong>precision</strong>:</p> \[\text{Pr}_{f,\mathbf{x}}(S) = \mathbb{P}_{\mathbf{y} \sim \mathcal{D}}[f(\mathbf{y}) = f(\mathbf{x}) ~|~ \mathbf{x}=\mathbf{y}].\] <p>For continuously valued fucntions \(f\) this can be further relaxed to being close to the original value in some fitting norm. One is then often interested in the most informative subset of a given maximal size:</p> \[S^* = \text{argmin}_{S: |S|\leq k} D_{f,\mathbf{x}}(S) \quad \text{where} \quad D_{f,\mathbf{x}}(S) = \|f(\mathbf{x}) - \mathbb{E}_{\mathbf{y}\sim \mathcal{D}|_{\mathbf{x}_S}}[f(\mathbf{y}) ]\|\] <p>In the language of Shapley values, we are looking for a small coalition that already achieves a value close to the whole set of players. There is a natural trade-off between the maximal set size \(k\) and the achievable distortion \(D(S^*)\).</p> <p>This concept can be refined without the arbitrariness of the norm by considering the mutual information.</p> <p><strong>Maximal Mutual Information</strong></p> <p>Mututal information measures the mutual dependence between two variables. In the context of the inpt features, it can be defined for a given subset S as as</p> \[I_{\mathbf{x} \sim \mathcal{D}}[f(\mathbf{x}); \mathbf{x}_S] = H_{\mathbf{x} \sim \mathcal{D}}[f(\mathbf{x})] - H_{\mathbf{x} \sim \mathcal{D}}[f(\mathbf{x}) ~|~ \mathbf{x}_S],\] <p>where \(H_{\mathbf{x} \sim \mathcal{D}}[f(\mathbf{x})]\) is the a priori entropy of the classification decision and \(H_{\mathbf{x} \sim \mathcal{D}}[f(\mathbf{x}) ~|~ \mathbf{x}_S]\) is the conditional entropy given the input set. When the conditional entropy is close to zero, the mutual information takes its maximal value as the pure a priori entropy.</p> \[H_{\mathbf{x} \sim \mathcal{D}}[f(\mathbf{x}) ~|~ \mathbf{x}_S] = - \sum_{l} p_l \log(p_l) \quad \text{where} \quad p_l = \mathbb{P}_{\mathbf{y} \sim \mathcal{D}}[f(\mathbf{y}) = l ~|~ \mathbf{y}_S = \mathbf{x}_S],\] <p>where \(l\) runs over the domain of \(f\). Similarly to the prime implicant explanations, one is often interested to find a small subset of the input that ensures high mutual information with the output:</p> \[S^* = \text{argmax}_{S: |S|\leq k} I_{\mathbf{x} \sim \mathcal{D}}[f(\mathbf{x}); \mathbf{x}_S].\] <h2 id="the-modelling-problem">The modelling Problem</h2> <p>All three presented methods to calculate the conditional probabilities \(\mathcal{D}_{\mathbf{x}_S}\) for all subsets \(S\) in question. For synthetic datasets these probabilities can be known, for realistic datasets however, these probabilities require explicit modelling of the conditional data distribution. This has been achieved practically with variational autoencoders or generative adversarial networks. Let us call these approximations \(\mathcal{D^\prime}|_{\mathbf{x}_S}\).</p> <h4 id="practical-problems">Practical Problems</h4> <p>There are basically two practical approaches to the modelling problem. The <strong>first</strong> is taking a simplified <strong>i.i.d. distribution</strong> (which is in particular independent of the given features):</p> \[\mathbb{P}_{\mathbf{y}\sim\mathcal{D}}(\mathbf{y}_{S^c} ~|~ \mathbf{y}_S = \mathbf{x}_S) = \prod_{i \in S^c} p(y_i).\] <p>This has been the approach taken for example in <a class="citation" href="#fong2017interpretable">(Fong &amp; Vedaldi, 2017)</a>,<a class="citation" href="#macdonald2019rate2">(MacDonald et al., 2019)</a> and <a class="citation" href="#ribeiro2016model">(Ribeiro et al., 2016)</a>. The problem here is that for certain masks this can create features that are not there in the original image, see Figure 4 for an illustration. This can actually happen even when unintended in case of an optimiser solving for small distortion \(D_{f,\mathbf{x}}\), as shown in Figure 4.</p> <div style="display: flex; justify-content: center;"> <embed src="https://swaeldchen.github.io/assets/img/merlin_arthur/bird_mask.png" alt="img1" style="float:center; margin-right: 1%; width:50%"></embed> <p style="clear: both;"></p> </div> <p class="figcap"><strong>Figure 4.</strong> The optimised mask to convince the classifier of the (correct) bird class constructs a feature that is not present in the original image, here a bird head looking to the left inside of the monochrome black wing of the original; from <a class="citation" href="#"></a>[Macdonald2021]. This can happen because of the effect explained in Figure 5 Left.</p> <p>In fact, these simplified models are the reason that the heuristic methods of LIME ans ShAP are manipulable as explained before. If they used a correct model of the data distribution, there would be no off-manifold inputs when calculating the importance valuese, and the trick to change the off-manifold behaviour of the classifier would be without effect.</p> <p>The <strong>second</strong>, data-driven approach is to train a <strong>generative model</strong> on the dataset:</p> \[\mathbb{P}_{\mathbf{y}\sim\mathcal{D}}(\mathbf{y}_{S^c} ~|~ \mathbf{y}_S = \mathbf{x}_S) = G(\mathbf{y}_{S^c}~;~ \mathbf{x}_{S}).\] <p>This has the advantage that the inpainting will likely be done more correctly thus evading the creation of new features by the mask. However, a new problem arises. Since it is likely that the classifier and the generator have been trained on the same dataset, they tend to learn the same biases which can cancel out and go undetected. An illustration is given in Figure 5 Right. The classifier learns to use water to identify ships. When a pixel mask containing the ship is selected, the generator paints the water back in, which can then be used by the classifier to answer correctly thus giving the appearance that the ship feature was used. This would give the ship high Shapley values and high mutual information, despite the classifier working in a way that will not generalise outside the dataset.</p> <div style="display: flex; justify-content: center;"> <embed src="https://swaeldchen.github.io/assets/img/merlin_arthur/failures.png" alt="img1" style="float:center; margin-right: 1%; width:50%"></embed> <p style="clear: both;"></p> </div> <p class="figcap"><strong>Figure 5.</strong> Different failure modes for different models of the data distribution. Both approaches have specific shortcomings. <em>Left:</em> Feature inpainting with an i.i.d. distribution. Selecting a mask can create a new feature that was not present in the original input. If one would consider a data-driven approach instead the rest of the image would likely be inpainted as black and the effect would disappear. <em>Right:</em> Data-driven inpainting. After selecting the boat feature, a trained generator inpaints the water back into the image, which the classifier uses for classification. Consequently the boat feature will get high Shaply Values/mutual information even though the classifier does not rely on boats. If one uses an i.i.d appraoch this effect would not appear.</p> <h4 id="theoretical-problems">Theoretical Problems</h4> <p>Since we want a formal approach with a bound on the calculated Shapley values, distortion or mutual information, we need a <strong>distance bound</strong> between \(\mathcal{D^\prime}|_{\mathbf{x}_S}\) and \(\mathcal{D}|_{\mathbf{x}_S}\) in some fitting norm, e.g. the total variation or Kullback-Leibler divergence</p> \[D_{\text{KL}}(\mathcal{D}|_{\mathbf{x}_S}, \mathcal{D^\prime}|_{\mathbf{x}_S}).\] <p>This is hard to achieve, since to establish such bounds one would need <strong>exponentially many samples</strong> from the dataset, since there are exponentially many subsets to condition on.</p> <p>Taking any image \(\mathbf{x}\) from ImageNet for example and conditioning on a subset \(S\) of pixels, there probably exists no second image with the same values on \(S\) when size of \(S\) is larger than 20. These conditional distributions thus cannot be sampled for most high-dimensional datasets and no quality bounds can be derived. <strong>One would need to trust one trained model to evaluate another trained model</strong> — this is a very strong condition for a formal guarantee!</p> <p>In the next post we discuss how this problem can be overcome by replacing the modelling of the data distribution with an adversarial setup.</p> <p><a href="/blog/2023/FI_start/">◀ Previous Post</a></p> <p align="right"><a href="/blog/2023/FI_Merlin-Arthur/"> Next Post ▶</a></p> <h3 id="references">References</h3> <ol class="bibliography"> <li> <div class="text-justify"> <span id="rudin2019stop">Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. <i>Nature Machine Intelligence</i>, <i>1</i>(5), 206–215.</span> </div> <div> </div> <div> </div> </li> <li> <div class="text-justify"> <span id="mohseni2021multidisciplinary">Mohseni, S., Zarei, N., &amp; Ragan, E. D. (2021). A multidisciplinary survey and framework for design and evaluation of explainable AI systems. <i>ACM Transactions on Interactive Intelligent Systems (TiiS)</i>, <i>11</i>(3-4), 1–45.</span> </div> <div> </div> <div> </div> </li> <li> <div class="text-justify"> <span id="lapuschkin2016analyzing">Lapuschkin, S., Binder, A., Montavon, G., Muller, K.-R., &amp; Samek, W. (2016). Analyzing classifiers: Fisher vectors and deep neural networks. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 2912–2920.</span> </div> <div> </div> <div> </div> </li> <li> <div class="text-justify"> <span id="lundberg2017unified">Lundberg, S. M., &amp; Lee, S.-I. (2017). A unified approach to interpreting model predictions. <i>Advances in Neural Information Processing Systems</i>, <i>30</i>.</span> </div> <div> </div> <div> </div> </li> <li> <div class="text-justify"> <span id="slack2020fooling">Slack, D., Hilgard, S., Jia, E., Singh, S., &amp; Lakkaraju, H. (2020). Fooling lime and shap: Adversarial attacks on post hoc explanation methods. <i>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</i>, 180–186.</span> </div> <div> </div> <div> </div> </li> <li> <div class="text-justify"> <span id="dimanov2020you">Dimanov, B., Bhatt, U., Jamnik, M., &amp; Weller, A. (2020). <i>You shouldn’t trust me: Learning models which conceal unfairness from multiple explanation methods.</i></span> </div> <div> </div> <div> </div> </li> <li> <div class="text-justify"> <span id="shapley1997value">Shapley, L. S. (1997). A value for n-person games. <i>Classics in Game Theory</i>, <i>69</i>.</span> </div> <div> </div> <div> </div> </li> <li> <div class="text-justify"> <span id="waldchen2022training2">Wäldchen, S., Pokutta, S., &amp; Huber, F. (2022). Training characteristic functions with reinforcement learning: Xai-methods play connect four. <i>International Conference on Machine Learning</i>, 22457–22474.</span> </div> <div> </div> <div> </div> </li> <li> <div class="text-justify"> <span id="brown2017adversarial">Brown, T. B., Mané, D., Roy, A., Abadi Martı́n, &amp; Gilmer, J. (2017). Adversarial patch. <i>ArXiv Preprint ArXiv:1712.09665</i>.</span> </div> <div> </div> <div> </div> </li> <li> <div class="text-justify"> <span id="waldchen2021computational2">Wäldchen, S., Macdonald, J., Hauch, S., &amp; Kutyniok, G. (2021). The computational complexity of understanding binary classifier decisions. <i>Journal of Artificial Intelligence Research</i>, <i>70</i>, 351–387.</span> </div> <div> </div> <div> </div> </li> <li> <div class="text-justify"> <span id="ribeiro2018anchors">Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2018). Anchors: High-precision model-agnostic explanations. <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, <i>32</i>(1).</span> </div> <div> </div> <div> </div> </li> <li> <div class="text-justify"> <span id="fong2017interpretable">Fong, R. C., &amp; Vedaldi, A. (2017). Interpretable explanations of black boxes by meaningful perturbation. <i>Proceedings of the IEEE International Conference on Computer Vision</i>, 3429–3437.</span> </div> <div> </div> <div> </div> </li> <li> <div class="text-justify"> <span id="macdonald2019rate2">MacDonald, J., Wäldchen, S., Hauch, S., &amp; Kutyniok, G. (2019). A rate-distortion framework for explaining neural network decisions. <i>ArXiv Preprint ArXiv:1905.11092</i>.</span> </div> <div> </div> <div> </div> </li> <li> <div class="text-justify"> <span id="ribeiro2016model">Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016). Model-agnostic interpretability of machine learning. <i>ArXiv Preprint ArXiv:1606.05386</i>.</span> </div> <div> </div> <div> </div> </li> </ol> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/FI_Merlin-Arthur/">Interactive Classification 2: Prover-Verifier Setup</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/FI_AFC/">Interactive Classification 3: Asymmetric Feature Correlation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/FI_Complexity/">Interactive Classification 5: Computational Complexity</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/FI_Relative_Strength/">Interactive Classification 4: Relative Strength of Cooperative and Adversarial Prover</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/AI_Safety/">AI safety - An Overview</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Stephan T. Wäldchen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>