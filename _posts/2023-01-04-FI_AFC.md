---
layout: post
title:  "Formal Interpretability 3: Asymmetric Feature Correlation"
date:   2023-01-04 00:00:00 +0100
categories: [research]
tags: [optimization, ml, uniform convexity, abstract]
math: true
comments: true
giscus_comments: true
author: Stephan WÃ¤ldchen
---

$\newcommand{\bfx}{\mathbf{x}}$
$\newcommand{\bfy}{\mathbf{y}}$
$\newcommand{\bfz}{\mathbf{z}}$
$\newcommand{\ap}{\text{Pr}}$
$\newcommand{\ekl}[1]{\mathopen{}\left[ #1 \right]\mathclose{}}$
$\newcommand{\E}{\mathbb{E}}$
$\renewcommand{\P}{\mathbb{P}}$
$\newcommand{\morg}{\widehat{M}}$
$\newcommand{\CA}{\mathcal{A}}$
$\newcommand{\CM}{\mathcal{M}}$

{% include abbrv.html %}

<style>
  .figcap {
    font-size: 0.9em;
  }
</style>

This post is part 3 of my series on <a href="/blog/2023/FI_start/">Formal Interpretability</a>.

*TL;DR:
We present interactive classification as an approach to define informative features without modelling the data distribution explicitly

1. Asymmetric Feature Correlation
1. Relative strength of cooperative and adversarial prover
1. Do we need a game equilibrium?
<!--more-->


### Asymmetric Feature Correlation

 AFC describes a possible quirk of datasets, where a set of features is strongly concentrated in a few data points in one class and spread out over almost all data points in another. We give an illustrative example in Figure 1.

<div style="display: flex; justify-content: center;">
  <img src="{{site.url }}{{site.baseurl }}/assets/img/merlin_arthur/afc.svg" alt="img1" style="float:center; width:100%">
  <p style="clear: both;"></p>
</div>

**Figure 1.** Example of a dataset an AFC $\kappa=6$. The ''fruit'' features are concentrated in one image for class $l=-1$ but spread out over six images for $l=1$ (vice versa for the ''fish'' features). Each individual feature is not indicative of the class as it appears exactly once in each class. Nevertheless, Arthur and Merlin can exchange ''fruits'' to indicate $l=1$ and ''fish'' for $l=-1$. The images where this strategy fails or can be exploited by Morgana are the two images on the left. Applyingour min-max theorem, we get $\epsilon_M = \frac{1}{7}$ and the set $D^{\prime}$ corresponds to all images with a single feature. Restricted to $D^{\prime}$, the features determine the class completely.
{:.figcap}

In this example, Merlin and Arthur can agree to exchange a fruit features to indicate class 1. Arthur can always be convinced by Merlin except in the one image with many fish. Likewise, the one image with the many fruit is the only one where he can be falsely convinced of class 1 by Morgana. This applies vice versa to class -1, where they exchange fish features. Thus the set $E_{M,\widehat{M},A}$ is only the two images on the left with the many features and

$$\epsilon_M = \frac{1}{7}.$$

But the features are individually uninformative! Each fish and fruit appear equally likely in both classes and thus $\ap_{\CD}(M)=\frac{1}{2}$. The bound

$$
  \ap_{\CD}(M) \geq 1-\epsilon_{M}
$$

thus needs to fail. In fact, $$\epsilon_M$$ can be made arbitrarily small as long as one can fit more features into the datapoints.
We save ourselves by definiting the set $D^\prime$, which in this case includes all the images with a single feature. It is easy to check that this set covers a $1-\epsilon_M$ portion of the original set and that conditioned on it the fish and fruit features determine the class completely.

If we want a bound that does not rely on a restricted set, we need to include the asymmetric feature correlation explicitely. We can formally define it as follows:

**Asymmetric Feature Correlation:** The AFC $\kappa$ of a dataset $D$ is defined as:

$$
\kappa = \max_{l\in \{-1,1\}} \max_{F \subset \Sigma} \E_{\bfy \sim \CD_l|_{F^*}}\ekl{\max_{\substack{\phi \in F \\ \text{s.t. }\bfy \in \phi}}\kappa_l(\phi, F)}
$$

with

$$
  \kappa_l(\phi, F) = \frac{\P_{\bfx \sim \CD_{-l}}\ekl{\bfx \in \phi  \,\middle|\, \bfx \in F^*}}{\P_{\bfx \sim \CD_l}\ekl{\bfx \in \phi \,\middle|\, \bfx \in F^*}}.
$$

where
$$F^\ast := \left\{\bfx \in D~|~ \exists~ \phi \in F: \phi \subseteq \bfx\right\}$$
is the set of all datapoins with a feature from $F$.

We give an **intuition** here: The probability
$$\P_{\bfx \sim \CD_{l}}\ekl{\bfx \in \phi \,\middle|\, \bfx \in F^*}$$
for $\phi\in F$
 is a measure of how correlated the features are. If all features appear in the same datapoints this quantity takes a maximal value of 1 for each $\bfz$. If no features share the same datapoint the value is minimally $\frac{1}{\bkl{F}}$ for the average $\bfz$.
The $\kappa_l(\bfz, F)$ thus measures the difference in correlation between the two classes. In the example in~\Cref{fig:afc} the worst-case $F$ for $l=-1$ correspond to the ''fish'' features and $\kappa_l(\bfz, F)=6$ for each feature.
To take an expectation over the features $\bfz$ requires a distribution, so we take the distribution of datapoints that have a feature from $F$, i.e. $\bfy \sim \CD_l|_{F^*}$, and select the worst-case feature from each datapoint. Then we maximise over class and the possible feature sets $F$.
Since, in Figure 5, the ''fish'' and ''fruit'' features are the worst case for each class respectively, we arrive at an AFC of 6.


### Relative Strength of Merlin and Morgana
Another important metric that we care about is the relative strength of the Merlin and Morgana classifiers. This is especially important if we intend to apply our setup to real data sets where Merlin and Morgana are not able to find the optimal features are every step.
We can relax thsi requirement in two important ways:
1. She only needs to find the same features that Merlin also uses
1. She only needs to do so at a success rate similar to Merlin

With this in mind, we define the notion of relative success rate as follows.

**Relative Success Rate:** Let $A\in \CA$ and $M, \morg \in \CM(D)$. Then the relative success rate $\alpha$ is defined as

$$
   \alpha := \min_{l\in \{-1,1\}} \frac{\P_{\bfx\sim \CD_{-l}}\ekl{A(\morg(\bfx))=l \,|\, \bfx \in F_l^\ast}}{\P_{\bfx\sim \CD_{l}}\ekl{A(M(\bfx))=l \,|\, \bfx \in F_l^\ast}},
$$

where
$$F_{l} := \{\phi \in \Sigma \,|\, \bfz\in M(D_l), A(\bfz)=l\}$$ is the set that Merlin uses to successfully convince Arthur of class $l$. In plain words: Given that the the datapoint has a feature that Merlin could successfully use, how likely is Morgana to discover this feature relative to Merlin?

It stands to reason that if both provers are implemented by the same algorithm, their performance should be similar. Of course, as with anything neural network related that might be hard to prove in practice.

### Key Result

With the above two notions of Relative Strength and Asymmetric Feature Correlation in mind, we can provide a key theoretical result of this paper.

**Main Theorem:** Let $D$ be two-class data space with AFC of $\kappa$ and class imbalance $B$. Let $A\in \CA$, and $M, \widehat{M}\in\CM(D)$ such that $\widehat{M}$ has a relative success rate of $\alpha$ with respect to $A, M$ and $D$.
Define

1. Completeness:
$$
\min\limits_{l \in \{-1,1\}}\P_{\bfx \sim \CD_l}\ekl{A(M(\bfx)) = c(\bfx) } \geq 1- \epsilon_c,
$$
2. Soundness
$$
\max\limits_{l \in \{-1,1\}} \P_{\bfx \sim \CD_l}\ekl{A(\morg(\bfx)) = -c(\bfx) } \leq  \epsilon_s.
$$

where $\CD_l$ is the data distribution conditioned on the class $l$.
Then it follows that

$$
\ap_{\CD}(M) \geq 1 - \epsilon_c - \frac{ \kappa \alpha^{-1}\epsilon_s}{1 - \epsilon_c+ \kappa \alpha^{-1}B^{-1}\epsilon_s}.
$$

This result shows that we can bound the performance of the feature selector Merlin (in terms of average precision) in the Merlin-Arthur framework using measurable metrics such as completeness and soundness.

### Key Takeaways
The above theoretical discussion shows two key contributions of our framework.
1. We do not assume our agents to be optimal. In the presented theorem Merlin is allowed to have any arbitrary strategy.
We rather rely on the relative strength of Merlin and Morgana for our bound. We also allow our provers to
select the features with the context of the full data point.

1. We do not make the assumption that features are independently distributed. Instead, we introduce the notion
of Asymmetric Feature Correlation (AFC) that captures which correlations make an information bound
difficult.




### References


{% bibliography --cited --file formal_interpretability.bib --template bib2 %}
