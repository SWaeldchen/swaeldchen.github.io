---
layout: post
title: <s>We are looking for a new PhD student in AI Interpretability</s>
date: 2023-04-01 16:11:00-0400
inline: false
related_posts: false
---

We are seeking a highly motivated and skilled research assistant to join our [Research Team](https://iol.zib.de/research/#LEARN) at IOL Lab at Zuse Institute Berlin. The successful candidate will work closely with our team on cutting-edge research projects in the field of Numerical Optimisation and Explainable AI.

Someone with a maths, theoretical physics or computer science background is preferred, but we are willing to consider anyone who can credibly demonstrate good reasoning and programming skills.

**Apply till <del>17th</del> 28th of May or until fitting candidate is found**

***
The position is part of the MATH+ project ["Expanding Merlin-Arthur Classifiers - Interpretable Neural Networks through Interactive Proof Systems"](https://mathplus.de/research-2/emerging-fields/ef1-extracting-dynamical-laws-from-complex-data/ef1-24/). This research project is part of the Emerging Fields Area "Extracting dynamical Laws from Complex Data". Our work focuses on the mathematical analysis of interpretability in AI systems. We want to investigate under which conditions interacting agents have to communicate honestly about their reasoning process. Our aim is to develop a theoretically sound foundation to make modern AI systems safe for deployment in sensitive ares. MATH+, the Berlin Mathematics Research Center, is a cross-institutional and interdisciplinary Cluster of Excellence. It sets out to explore and further develop new approaches in application-oriented mathematics. For more information see: <https://mathplus.de>.

***
#### **Broad Research Direction:**

We want to build a theory that guarantees Interpretability (e.g. in the form of information bounds) for modern AI systems. Can we play AI agents against each other, so they are forced to cooperate with humans? Is it possible that AI systems prove the soundness of their reasoning to us? Can we give interpretations in text form? How robust are these approaches? Do the explanations degenerate when the AIs are incentivised to obscure their reasoning? Can we use a form of error-correction to prevent this?

For preliminary results see our [paper](https://arxiv.org/pdf/2206.00759.pdf). See <a href="/projects/IP_PSPACE">here</a> for one possible angle for this project.

***
#### **Requirements:**
1. completed university degree (Diplom, Master or equivalent) in Mathematics, Computer Science or a closely related course of studies,
2. experience in theoretical research (e.g. proving mathematical theorems),
3. solid knowledge of probability theory,
4. good programming skills (Python),
5. knowledge in computational complexity theory, graph theory and game theory are an advantage,
6. fluent in English,
7. independent thinker.

To apply, write to **(aschulz@math.tu-berlin.de)**. Write me **(waeldchen@zib.de)** for further information. If you know a candidate that might be a good fit for this position, please forward them this announcement.

### **Please Include:**
1. how you fulfill the requirements,
2. how you found out about this position
3. link to your website, previous projects or papers,
4. whether you can physically be present at our Berlin office,
5. what your interests are and what motivates you for this position.

 [Official TUB Announcement (German)](https://www.jobs.tu-berlin.de/stellenausschreibungen/166084)
